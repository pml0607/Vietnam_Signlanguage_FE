# Vietnam Sign Language Recognition System

Há»‡ thá»‘ng nháº­n dáº¡ng ngÃ´n ngá»¯ kÃ½ hiá»‡u Viá»‡t Nam sá»­ dá»¥ng nhiá»u mÃ´ hÃ¬nh deep learning bao gá»“m S3D, VideoMAE vÃ  cÃ¡c ká»¹ thuáº­t xá»­ lÃ½ video tiÃªn tiáº¿n.

## ğŸ“‹ Má»¥c lá»¥c

- [CÃ i Ä‘áº·t mÃ´i trÆ°á»ng](#cÃ i-Ä‘áº·t-mÃ´i-trÆ°á»ng)
- [Cáº¥u trÃºc dá»± Ã¡n](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [Chuáº©n bá»‹ dá»¯ liá»‡u](#chuáº©n-bá»‹-dá»¯-liá»‡u)
- [Training cÃ¡c mÃ´ hÃ¬nh](#training-cÃ¡c-mÃ´-hÃ¬nh)
- [Inference](#inference)
- [Há»‡ thá»‘ng Watcher](#há»‡-thá»‘ng-watcher)

## ğŸ› ï¸ CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

### YÃªu cáº§u há»‡ thá»‘ng
- Python >= 3.9
- PyTorch >= 1.13.1+cu116
- Torchvision >= 1.14.1
- CUDA 11.0+ (cho GPU training)
- RAM >= 16GB
- GPU memory >= 8GB (khuyáº¿n nghá»‹)

### CÃ i Ä‘áº·t dependencies

```bash
# Clone repository
git clone https://github.com/pml0607/Vietnam_Signlanguage_FE.git
cd Vietnam_Signlanguage_FE

# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
pip install -r requirements.txt

# CÃ i Ä‘áº·t thÃªm transformers cho VideoMAE
pip install transformers
pip install pytorchvideo
pip install fire
pip install watchdog
```

### Cáº¥u hÃ¬nh CUDA
```bash
# Kiá»ƒm tra CUDA
nvidia-smi
python -c "import torch; print(torch.cuda.is_available())"
```

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
Vietnam_Signlanguage_FE/
â”œâ”€â”€ Configurate/           # File cáº¥u hÃ¬nh training
â”‚   â”œâ”€â”€ train.yaml         # Cáº¥u hÃ¬nh S3D training
â”‚   â”œâ”€â”€ landmark_config.yaml
â”‚   â”œâ”€â”€ segmentation.yaml  # Cáº¥u hÃ¬nh segmentation
â”‚   â”œâ”€â”€ data_preprocess.yaml
â”‚   â””â”€â”€ validate.yaml
â”œâ”€â”€ S3D/                   # MÃ´ hÃ¬nh S3D (3D CNN)
â”‚   â”œâ”€â”€ train.py           # Training script
â”‚   â”œâ”€â”€ model.py           # Kiáº¿n trÃºc mÃ´ hÃ¬nh
â”‚   â”œâ”€â”€ dataset.py         # Data loader
â”‚   â”œâ”€â”€ preprocess_sliding.py  # Tiá»n xá»­ lÃ½ RGB
â”‚   â””â”€â”€ validate.py        # Evaluation script
â”œâ”€â”€ Transformer/           # CÃ¡c mÃ´ hÃ¬nh Transformer
â”‚   â”œâ”€â”€ rgb/               # VideoMAE RGB only
â”‚   â”œâ”€â”€ rgb_landmark/      # VideoMAE RGB + Landmark
â”‚   â”œâ”€â”€ rgb_landmark_v2/   # PhiÃªn báº£n cáº£i tiáº¿n (khuyáº¿n nghá»‹)
â”‚   â””â”€â”€ 6channel/          # 6-channel fusion
â”œâ”€â”€ Source/                # Há»‡ thá»‘ng inference realtime
â”‚   â”œâ”€â”€ data/              # ThÆ° má»¥c dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ rgb/           # Video input
â”‚   â”‚   â”œâ”€â”€ npy/           # Landmark data
â”‚   â”‚   â”œâ”€â”€ cache/         # RGB tensor cache
â”‚   â”‚   â””â”€â”€ history/       # Processed files
â”‚   â”œâ”€â”€ Watcher_*.py       # CÃ¡c watcher component
â”‚   â””â”€â”€ run_watcher.py     # Script cháº¡y há»‡ thá»‘ng
â”œâ”€â”€ Ultralytics/           # YOLO segmentation
â”‚   â”œâ”€â”€ segmentation.py    # TÃ¡ch ná»n
â”‚   â””â”€â”€ add_bg.py          # ThÃªm background
â”œâ”€â”€ Mediapipe/            # MediaPipe landmark extraction
â””â”€â”€ requirements.txt      # Python dependencies
```

## ğŸ“Š Chuáº©n bá»‹ dá»¯ liá»‡u

### 1. Cáº¥u trÃºc dá»¯ liá»‡u ban Ä‘áº§u

Tá»• chá»©c dá»¯ liá»‡u thÃ´ theo cáº¥u trÃºc sau:

```
dataset/
â”œâ”€â”€ A1P1/
â”‚   â””â”€â”€ rgb/
â”‚       â”œâ”€â”€ file1.avi
â”‚       â”œâ”€â”€ file2.avi
â”‚       â””â”€â”€ ...
â”œâ”€â”€ A1P2/
â”‚   â””â”€â”€ rgb/
â”‚       â”œâ”€â”€ file1.avi
â”‚       â””â”€â”€ ...
â””â”€â”€ ...
```

### 2. Segmentation vÃ  thÃªm background (tÃ¹y chá»n)

Náº¿u dataset thiáº¿u Ä‘a dáº¡ng background, thá»±c hiá»‡n cÃ¡c bÆ°á»›c sau:

#### Cáº¥u hÃ¬nh segmentation
Chá»‰nh sá»­a `Configurate/segmentation.yaml`:
```yaml
video_root: "/path/to/raw/dataset"
output_root: "/path/to/segmented"
model_path: "Ultralytics/yolo11l-seg.pt"
```

#### Cháº¡y segmentation
```bash
python Ultralytics/segmentation.py
python Ultralytics/add_bg.py
```

Káº¿t quáº£:
```
segmented/
â”œâ”€â”€ bitwised/    # Video Ä‘Ã£ tÃ¡ch ná»n
â””â”€â”€ mask/        # Mask files

video_with_random_background/
â”œâ”€â”€ A1P1/
â”‚   â””â”€â”€ rgb/
â””â”€â”€ ...          # Video vá»›i background ngáº«u nhiÃªn
```

### 3. Tiá»n xá»­ lÃ½ cho training

#### Cáº¥u hÃ¬nh data preprocessing
Chá»‰nh sá»­a `Configurate/data_preprocess.yaml`:
```yaml
input_root: "/path/to/processed/dataset"
output_root: "/path/to/preprocessed"
clip_length: 16
stride: 8
```

#### Táº¡o training clips
```bash
# Cho dataset 3 channel (RGB)
python S3D/preprocess_sliding.py

# Cho dataset 6 channel (RGB + Flow)
python S3D/preprocess_sliding_6ch.py
```

### 4. Táº¡o landmark data (cho Transformer models)

```bash
cd Source/
python Watcher_Landmark.py \
    --input_dir /path/to/videos \
    --output_dir /path/to/landmarks \
    --config_path model/wholebody_w48_384x384_adam_lr1e-3.yaml \
    --checkpoint_path model/wholebody_hrnet_w48_384x384.pth
```

## ğŸš€ Training cÃ¡c mÃ´ hÃ¬nh

### 1. Training S3D Model

#### Cáº¥u hÃ¬nh training
Chá»‰nh sá»­a `Configurate/train.yaml`:

```yaml
paths:
  train_dir: "/path/to/preprocessed/train"
  val_dir: "/path/to/preprocessed/val"
  log_dir: "runs/s3d_experiment"
  best_model_path: "/path/to/save/best_model.pt"

training:
  batch_size: 4
  epochs: 50
  learning_rate: 0.0001
  num_classes: 120  # Sá»‘ lÆ°á»£ng classes
  freeze_until_layer: 10

model:
  pretrained: true
  in_channels: 3

dataloader:
  num_workers: 4

augmentation:
  single_stream:
    enable: true
  dual_stream:
    enable: false
```

#### Cháº¡y training
```bash
cd S3D/
python train.py --config ../Configurate/train.yaml
```

#### Distributed training (multi-GPU)
```bash
python -m torch.distributed.launch --nproc_per_node=2 train.py
```

### 2. Training VideoMAE Models

#### RGB only model
```bash
cd Transformer/rgb/
python train.py \
    --dataset_root_path /path/to/dataset \
    --model_ckpt MCG-NJU/videomae-base \
    --num_epochs 30 \
    --batch_size 8
```

#### RGB + Landmark model (Khuyáº¿n nghá»‹)
```bash
cd Transformer/rgb_landmark_v2/
python train.py \
    --dataset_root_path /path/to/dataset \
    --model_ckpt MCG-NJU/videomae-base \
    --num_epochs 30 \
    --batch_size 4
```

#### 6-channel model
```bash
cd Transformer/6channel/
python train.py \
    --dataset_root_path /path/to/dataset \
    --model_ckpt MCG-NJU/videomae-base \
    --num_epochs 30 \
    --batch_size 2
```

### 3. Fine-tuning tá»« pretrained model

```bash
python train.py \
    --dataset_root_path /path/to/new/dataset \
    --model_ckpt /path/to/pretrained/checkpoint \
    --num_epochs 10 \
    --batch_size 4 \
    --learning_rate 1e-5
```

## ğŸ” Inference

### 1. Single video inference

```bash
cd Transformer/rgb_landmark_v2/
python test.py \
    --model_path /path/to/checkpoint \
    --video_path /path/to/video.avi \
    --landmark_path /path/to/landmark.npy
```

### 2. Batch inference

```bash
python inference.py \
    --model_path /path/to/checkpoint \
    --data_dir /path/to/test/data \
    --output_dir /path/to/results
```

### 3. S3D inference

```bash
cd S3D/
python validate.py \
    --model_path /path/to/model.pt \
    --test_dir /path/to/test/data \
    --config ../Configurate/validate.yaml
```

## âš¡ Há»‡ thá»‘ng Watcher (Real-time Processing)

Há»‡ thá»‘ng watcher tá»± Ä‘á»™ng xá»­ lÃ½ video má»›i vÃ  thá»±c hiá»‡n inference real-time vá»›i kháº£ nÄƒng auto-cleanup.

### Cáº¥u trÃºc há»‡ thá»‘ng
- **Watcher_Landmark.py**: Táº¡o landmark tá»« video sá»­ dá»¥ng HRNet
- **Watcher_Inference.py**: Thá»±c hiá»‡n inference vá»›i VideoMAE
- **run_watcher.py**: Äiá»u phá»‘i toÃ n bá»™ há»‡ thá»‘ng vá»›i polling
- **Auto-cleanup**: Tá»± Ä‘á»™ng di chuyá»ƒn file Ä‘Ã£ xá»­ lÃ½ vÃ o history

### 1. Thiáº¿t láº­p thÆ° má»¥c

```bash
mkdir -p Source/data/{rgb,npy,cache,history/{rgb,npy,cache}}
mkdir -p Source/results
```

### 2. Cáº¥u hÃ¬nh model paths

Chá»‰nh sá»­a cÃ¡c Ä‘Æ°á»ng dáº«n trong `Source/run_watcher.py`:

```python
# Landmark model
config_path="/path/to/wholebody_w48_384x288.yaml"
checkpoint_path="/path/to/hrnet_w48_coco_wholebody_384x288-6e061c6a_20200922.pth"

# Inference model  
model_ckpt_path="/path/to/videomae-base-finetuned_47classes/checkpoint-330"
dataset_root_path="/path/to/dataset/for/label/mapping"
```

### 3. Cháº¡y há»‡ thá»‘ng watcher

#### Cháº¿ Ä‘á»™ tá»•ng há»£p (Combined mode) - Khuyáº¿n nghá»‹
```bash
cd Source/
python run_watcher.py
```

#### Cháº¿ Ä‘á»™ Ä‘Æ¡n láº»
```bash
# Chá»‰ landmark generation
WATCHER_TYPE=landmark python run_watcher.py

# Chá»‰ inference
WATCHER_TYPE=inference python run_watcher.py
```

#### SLURM environment
```bash
# Submit job
sbatch --job-name=watcher \
       --gres=gpu:1 \
       --mem=16G \
       --wrap="cd Source && python run_watcher.py"
```

### 4. Sá»­ dá»¥ng há»‡ thá»‘ng

1. **Upload video**: Äáº·t file `.avi` vÃ o thÆ° má»¥c `Source/data/rgb/`
2. **Tá»± Ä‘á»™ng xá»­ lÃ½**: 
   - Há»‡ thá»‘ng tá»± Ä‘á»™ng detect video má»›i
   - Táº¡o landmark (.npy) trong `data/npy/`
   - Táº¡o RGB cache trong `data/cache/`
   - Thá»±c hiá»‡n inference
   - LÆ°u káº¿t quáº£ JSON vÃ o `results/`
3. **Auto-cleanup**: File Ä‘Ã£ xá»­ lÃ½ tá»± Ä‘á»™ng di chuyá»ƒn vÃ o `history/`
   - `history/rgb/` - Video files
   - `history/npy/` - Landmark files  
   - `history/cache/` - Cache files

### 5. Monitoring vÃ  logs

```bash
# Theo dÃµi real-time logs
tail -f watcher.log

# Check thread status
grep "MONITOR" watcher.log

# Check processing results
ls Source/results/*.json
```

### 6. Káº¿t quáº£ inference

Má»—i video Ä‘Æ°á»£c xá»­ lÃ½ sáº½ táº¡o file JSON káº¿t quáº£:
```json
{
  "id": "video_001",
  "status": "completed",
  "prediction": {
    "class_id": 15,
    "class_name": "xin_chao"
  },
  "processing_time": 2.34,
  "timestamp": 1640995200.0,
  "video_tensor_file": "video_001.npy",
  "landmark_file": "video_001.npy",
  "landmark_shape": [16, 543, 3]
}
```

## ğŸ“ˆ Evaluation vÃ  Validation

### 1. ÄÃ¡nh giÃ¡ S3D model

```bash
cd S3D/
python validate.py \
    --model_path /path/to/model.pt \
    --test_dir /path/to/test/data \
    --config ../Configurate/validate.yaml
```


## ğŸ“š TÃ i liá»‡u tham kháº£o

- [VideoMAE Paper](https://arxiv.org/abs/2203.12602)
- [S3D Paper](https://arxiv.org/abs/1712.04851)
- [HRNet Paper](https://arxiv.org/abs/1908.07919)
- [MediaPipe Holistic](https://google.github.io/mediapipe/solutions/holistic.html)
- [YOLOv11 Documentation](https://docs.ultralytics.com/)


## ğŸ“„ License

Distributed under the MIT License. See `LICENSE` for more information.

## ğŸ“§ LiÃªn há»‡

- Project Link: [https://github.com/pml0607/Vietnam_Signlanguage_FE](https://github.com/pml0607/Vietnam_Signlanguage_FE)
- Issues: [https://github.com/pml0607/Vietnam_Signlanguage_FE/issues](https://github.com/pml0607/Vietnam_Signlanguage_FE/issues)

---

## ğŸ”„ Workflow tá»•ng quan

```mermaid
graph LR
    A[Raw Videos] --> B[Segmentation]
    B --> C[Background Addition]
    C --> D[Preprocessing]
    D --> E[Training]
    E --> F[Model Checkpoint]
    
    G[New Video] --> H[Watcher System]
    H --> I[Landmark Generation]
    H --> J[RGB Cache]
    I --> K[Inference]
    J --> K
    K --> L[Results]
    L --> M[Auto Cleanup]
```

**Happy coding! ğŸš€**